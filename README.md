# üéß Speech-To-Text - TP Jour 1

Ce projet impl√©mente un pipeline complet de Speech-to-Text (STT) de A √† Z, en explorant diff√©rentes architectures de deep learning.

## üìã Table des mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Installation](#installation)
- [Structure du projet](#structure-du-projet)
- [Parties du TP](#parties-du-tp)
- [Utilisation](#utilisation)
- [Ressources](#ressources)

## üéØ Vue d'ensemble

Ce TP couvre les aspects suivants du Speech-to-Text :

1. **Partie 1** : MLP + MFCC + CTC Loss
2. **Partie 2** : CNN + Spectrogrammes
3. **Partie 3** : RNN (LSTM/GRU/BiLSTM)
4. **Partie 4** : Transformers et Conformer
5. **Partie 5** : Optimisation d'hyperparam√®tres (Optuna)

## üîß Installation

### Pr√©requis

- Python 3.8+
- GPU recommand√© (mais fonctionne aussi sur CPU)

### Installation des d√©pendances

```bash
# Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### Installation avec GPU (CUDA)

```bash
# Pour CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Pour CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## üìÅ Structure du projet

```
Speech_to_text_project/
‚îú‚îÄ‚îÄ README.md                          # Ce fichier
‚îú‚îÄ‚îÄ requirements.txt                   # D√©pendances Python
‚îú‚îÄ‚îÄ TP STT.md                         # Sujet du TP (format markdown)
‚îú‚îÄ‚îÄ TP STT.pdf                        # Sujet du TP (format PDF)
‚îÇ
‚îú‚îÄ‚îÄ part1_mlp_mfcc_ctc.py             # Partie 1: MLP + MFCC + CTC
‚îú‚îÄ‚îÄ part2_cnn_spectrogram.py          # Partie 2: CNN + Spectrogrammes
‚îú‚îÄ‚îÄ part3_rnn_lstm.py                 # Partie 3: RNN (LSTM/GRU/BiLSTM)
‚îú‚îÄ‚îÄ part4_transformer.py              # Partie 4: Transformers
‚îî‚îÄ‚îÄ part5_hyperparameter_tuning.py   # Partie 5: Tuning hyperparam√®tres
```

## üöÄ Parties du TP

### Partie 1 : MLP + MFCC + CTC

**Objectif** : Construire un pipeline STT minimal avec MLP.

**Features** :
- Extraction de MFCC (Mel-Frequency Cepstral Coefficients)
- Encodage caract√®re par caract√®re
- Architecture MLP simple
- Loss CTC (Connectionist Temporal Classification)

**Ex√©cution** :
```bash
python part1_mlp_mfcc_ctc.py
```

**R√©sultats** :
- `part1_model.pth` : Mod√®le entra√Æn√©
- `part1_training_curves.png` : Courbes d'apprentissage

### Partie 2 : CNN + Spectrogrammes

**Objectif** : Am√©liorer l'extraction de features avec des CNN.

**Nouveaut√©s** :
- Remplacement MFCC ‚Üí Mel-Spectrogramme
- Couches convolutionnelles pour extraction de features
- Comparaison performances avec Partie 1

**Ex√©cution** :
```bash
python part2_cnn_spectrogram.py
```

**R√©sultats** :
- `part2_model.pth` : Mod√®le CNN
- `part2_training_curves.png` : Courbes d'apprentissage

### Partie 3 : RNN (LSTM/GRU/BiLSTM)

**Objectif** : Explorer les architectures r√©currentes pour capturer la temporalit√©.

**Architectures test√©es** :
- LSTM (Long Short-Term Memory)
- GRU (Gated Recurrent Unit)
- BiLSTM (Bidirectional LSTM)
- CNN + LSTM hybride

**Ex√©cution** :
```bash
python part3_rnn_lstm.py
```

**R√©sultats** :
- Mod√®les pour chaque architecture
- `part3_architecture_comparison.png` : Comparaison des architectures

**Aspects √©tudi√©s** :
- Capacit√© temporelle
- Stabilit√© de la CTC loss
- Vitesse d'entra√Ænement

### Partie 4 : Transformers

**Objectif** : Impl√©menter une architecture Transformer pour ASR.

**Features** :
- Self-attention sur frames audio
- Positional encoding
- Architecture Transformer classique
- Architecture Conformer (Convolution-augmented Transformer)

**Ex√©cution** :
```bash
python part4_transformer.py
```

**R√©sultats** :
- `part4_model_transformer.pth` : Mod√®le Transformer
- `part4_model_conformer.pth` : Mod√®le Conformer
- `part4_transformer_comparison.png` : Comparaison

### Partie 5 : Optimisation d'hyperparam√®tres

**Objectif** : Trouver les meilleurs hyperparam√®tres avec Optuna.

**Hyperparam√®tres optimis√©s** :
- Type de features (MFCC vs MelSpec)
- Nombre de features
- Architecture (LSTM vs GRU)
- Taille des couches cach√©es
- Nombre de couches
- Dropout
- Learning rate
- Batch size
- Augmentation audio (niveau de bruit)

**Ex√©cution** :
```bash
# Optimisation avec Optuna (recommand√©)
python part5_hyperparameter_tuning.py

# Grid Search (alternative)
python part5_hyperparameter_tuning.py --grid
```

**R√©sultats** :
- `part5_best_params.json` : Meilleurs hyperparam√®tres trouv√©s
- `part5_tuning_summary.png` : R√©sum√© de l'optimisation
- `part5_optimization_history.png` : Historique (si plotly install√©)
- `part5_param_importances.png` : Importance des param√®tres (si plotly install√©)

## üíª Utilisation

### Entra√Ænement rapide

Chaque script peut √™tre ex√©cut√© ind√©pendamment :

```bash
# Partie 1
python part1_mlp_mfcc_ctc.py

# Partie 2
python part2_cnn_spectrogram.py

# Partie 3
python part3_rnn_lstm.py

# Partie 4
python part4_transformer.py

# Partie 5
python part5_hyperparameter_tuning.py
```

### Donn√©es

Par d√©faut, les scripts g√©n√®rent des **donn√©es audio synth√©tiques** pour tester rapidement les architectures. Les donn√©es sont cr√©√©es dans le dossier `data/dummy/`.

### Utiliser vos propres donn√©es

Pour utiliser vos propres donn√©es audio, modifiez la fonction `create_dummy_data()` dans chaque script :

```python
# Remplacer
audio_paths, transcripts = create_dummy_data(num_samples=100)

# Par vos propres donn√©es
audio_paths = ['chemin/vers/audio1.wav', 'chemin/vers/audio2.wav', ...]
transcripts = ['transcription 1', 'transcription 2', ...]
```

Formats audio support√©s : WAV, MP3, FLAC, OGG

### Datasets recommand√©s

Pour des exp√©riences r√©elles, utilisez ces datasets :

- **LibriSpeech** : [http://www.openslr.org/12/](http://www.openslr.org/12/)
- **Common Voice** : [https://commonvoice.mozilla.org/](https://commonvoice.mozilla.org/)
- **TIMIT** : [https://catalog.ldc.upenn.edu/LDC93S1](https://catalog.ldc.upenn.edu/LDC93S1)
- **VoxForge** : [http://www.voxforge.org/](http://www.voxforge.org/)

## üìä R√©sultats attendus

Les scripts g√©n√®rent automatiquement :

1. **Mod√®les entra√Æn√©s** (fichiers `.pth`)
2. **Courbes d'apprentissage** (fichiers `.png`)
3. **Pr√©dictions d'exemple** (affich√©es dans le terminal)
4. **Comparaisons** entre architectures

### Exemple de sortie

```
Epoch 15/20 - Train Loss: 2.3456, Val Loss: 2.4567

Sample predictions:
  True: 'hello world'
  Pred: 'helo world'
  
  True: 'deep learning'
  Pred: 'deep learning'
```

## üî¨ Concepts cl√©s

### MFCC vs Mel-Spectrogram

- **MFCC** : Coefficients cepstraux, repr√©sentation compacte
- **Mel-Spectrogram** : Repr√©sentation temps-fr√©quence, plus d'information

### CTC Loss

La **Connectionist Temporal Classification** permet d'aligner automatiquement les s√©quences audio et texte sans annotation temporelle pr√©cise.

**Avantages** :
- Pas besoin d'alignement manuel
- G√®re des s√©quences de longueurs diff√©rentes
- Token "blank" pour g√©rer les silences

### Architectures

| Architecture | Avantages | Inconv√©nients |
|-------------|-----------|---------------|
| **MLP** | Simple, rapide | Pas de mod√©lisation temporelle |
| **CNN** | Extraction de features locales | Champ r√©ceptif limit√© |
| **LSTM/GRU** | Mod√©lisation temporelle | S√©quentiel, lent |
| **Transformer** | Parall√©lisable, long contexte | Co√ªteux en m√©moire |
| **Conformer** | Combine CNN et attention | Complexe |

## üìö Ressources

### Documentation officielle

- **PyTorch Audio** : [https://pytorch.org/audio/](https://pytorch.org/audio/)
- **Librosa** : [https://librosa.org/](https://librosa.org/)
- **Optuna** : [https://optuna.org/](https://optuna.org/)

### Tutoriels

- **MFCC vs Mel-Spectrogram** : [https://vtiya.medium.com/mfcc-vs-mel-spectrogram-8f1dc0abbc62](https://vtiya.medium.com/mfcc-vs-mel-spectrogram-8f1dc0abbc62)
- **Keras CTC ASR** : [https://keras.io/examples/audio/ctc_asr/](https://keras.io/examples/audio/ctc_asr/)
- **Understanding CTC** : [https://distill.pub/2017/ctc/](https://distill.pub/2017/ctc/)
- **Transformer ASR** : [https://keras.io/examples/audio/transformer_asr/](https://keras.io/examples/audio/transformer_asr/)
- **HF Audio Course** : [https://huggingface.co/learn/audio-course/](https://huggingface.co/learn/audio-course/)

### Papers

- **CTC** : Graves et al., "Connectionist Temporal Classification"
- **wav2vec 2.0** : Baevski et al., 2020
- **Conformer** : Gulati et al., 2020
- **Whisper** : Radford et al., 2022

## üõ†Ô∏è D√©pannage

### Probl√®me : CUDA out of memory

**Solution** : R√©duire le batch size

```python
train_loader = DataLoader(..., batch_size=4)  # Au lieu de 8
```

### Probl√®me : CTC Loss = inf ou nan

**Solutions** :
1. V√©rifier que `feature_lengths > transcript_lengths`
2. Utiliser `zero_infinity=True` dans CTCLoss
3. R√©duire le learning rate
4. Ajouter gradient clipping

### Probl√®me : Pas de GPU d√©tect√©

**V√©rification** :
```python
import torch
print(torch.cuda.is_available())  # Devrait √™tre True
print(torch.cuda.get_device_name(0))  # Nom de votre GPU
```

### Probl√®me : Import error pour torchaudio

**Solution** :
```bash
pip uninstall torchaudio
pip install torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## üéì Extensions possibles

Pour aller plus loin apr√®s le TP :

1. **Beam Search Decoding** : Am√©liorer le d√©codage CTC
2. **Language Model** : Ajouter un mod√®le de langue pour corriger les pr√©dictions
3. **Data Augmentation** : SpecAugment, time stretching, pitch shifting
4. **Multi-GPU Training** : Distributed Data Parallel
5. **Quantization** : Optimiser pour l'inf√©rence
6. **ONNX Export** : D√©ploiement optimis√©

## üìù Notes

- Les donn√©es synth√©tiques sont g√©n√©r√©es al√©atoirement et ne permettent pas d'√©valuer les performances r√©elles
- Pour des r√©sultats significatifs, utilisez des datasets r√©els (LibriSpeech, Common Voice, etc.)
- Les architectures sont simplifi√©es pour des raisons p√©dagogiques
- Les mod√®les state-of-the-art utilisent des architectures beaucoup plus grandes

## ü§ù Contribution

Ce projet est un TP √©ducatif. Pour toute question ou suggestion :
- Consultez le fichier `TP STT.md` pour plus de d√©tails
- R√©f√©rez-vous aux ressources list√©es ci-dessus

## üìÑ Licence

Ce projet est fourni √† des fins √©ducatives.

---

**Bon courage pour le TP ! üöÄ**

